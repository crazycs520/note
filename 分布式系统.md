* [分布式Quorum机制,NWR策略读写模型](http://www.voidcn.com/article/p-qaphhdsx-bmr.html)

# #分布式存储

## GFS(2003)

- 设计模型背景

  - 认为组件失败是常事，并不是异常。因为GFS部署在成百上千台的普通计算机上。磁盘，内存，路由器，网络，电源；代码，操作系统等都有可能出错。
  - 每个文件都很大（几个G），每个文件都包含多个应用对象。也支持小文件，但效率不高，没对小文件做优化。
  - 大部分文件操作都是 append 而不是修改已存在的数据。
  - 主要读负载有2种：large streaming reads 和 small random reads。如果有多个小的随机读取，应考虑batch并sort。
  - 主要写负载是sequential writes that append data to files。也支持随机写，但效率不高。
  - 放松了一致性模型的要求，提供原子append API保证并发append。
  - 并发append的效率要高，保证最小的同步开销很关键。
  - 高持久的带宽比第低延迟更重要。应为处理的数据一般较大。

- 提供的API

  - create, delete,open, close, read, and write files.
  - snapshot ,
  -  record append operations. It is useful for implementing multi-way merge results and producerconsumer queues that many clients can simultaneously append to without additional locking.

- 架构： 单个Master（单点问题？）和多个chunkservers。

  - 适合存储大文件，每个文件分为多个chunk存储。
  -  每个chunk 的大小是 64 MB（内存碎片），且在创建时由master负责分配一个全局唯一的64bit的 handle assigned。默认每个chunk存3份副本，用户可配置。
    - 大chunk降低了对maste的访问，对master性能瓶颈改善非常的关键。
    - 大chunk有助于client和chunkserver之间保持TCP长连接。
    - 大chunk减少了master元数据的存储，使得master可以把 metadata存放在内存。
    - 大chunk也有缺点，比如导致某些chunk成为热点数据。但是对google 使用场景来说，这个情况很少出现，因为大多数读是multi-chunk files read。不过也出现了这种热点情况，采取的策略是增加热点chunk的复制因子。
  - Master 节点管理所有的文件系统元信息（namespace , 访问控制信息，文件和chunk的映射，chunk的位置信息）, chunk 的位置信息在 master启动时轮询 chunkserver (启动后怎么知道 chunkserver 的IP？), 不用持久化。
  - client从master读取到metadata 后直接和到chunkserver读取数据。

  <img src="/Users/cs/note/images/gfs01.png" style="zoom:50%" />

- master保存的metadata存在内存中，  包括

  - the file and chunk namespaces，需要做持久化，存在operation log 里
  - the mapping from files to chunks ， 需要做持久化
  - the locations of each chunk’s replicas，不用持久化，master每次启动和有新的chunkserver加入集群时主动询问chunkserver 存储的chunks 信息。这样可以使得master在 chunkserver 宕机或者加入时保持信息同步。

- metadata保存在内存中，master可以高效的扫描整个metadata 来实现 chunk的垃圾回收，chunk的迁移实现负载均衡，如果chunkserver宕机后chunk的重新复制，和整个chunkserver集群的磁盘使用状况。

- client 可以缓存数据元信息一定时间。Chunkserver 不需要缓存文件数据 ， 因为 文件 以本地方式保存， Linux 会把经常访问的数据缓存在内存中。


- Master 和 chunksever 有周期性的 心跳检测。
- Operation Log 非常关键，不仅持久化保存了metadata，而且，还是并发操作时的逻辑时间线。所以，master在返回给client前，会把Log复制到远程并都持久化到本地，然后才返回给客户端。用batch提到吞吐量。
  - Master恢复GFS文件系统通过replay Operation Log。为缩小恢复时间，需要定期checkpoint。checkpoint是一个压缩的B树，可以直接映射到内存。
  - 在做checkpoint时，另起一个线程，并且master重新切换到一个新的日志文件，这样checkpoint时也能继续进行操作。checkpoint的完成是指远程和本地都已经持久化数据到磁盘。
  - checkpoint即使失败也不影响系统的正确性。
- GFS放松了一致性模型来保证高可用和便于实现。
  - File namespace 修改（如 创建）是原子的，由namespace locking保证原子性和正确性。由master 操作 operation log保证操作的顺序性。
  - 用chunk version numbers 来探测副本是否丢失更改而导致过期。过期的chunk会被垃圾回收。

<img src="/Users/cs/note/images/gfs02.png" style="zoom:50%" />

- 因为所有对chunk的修改都要同步到所有副本上。所以Master 为 chunk的一个副本建立 lease 作为主chunk , 初始Lease为60秒。由主chunk接受所有的修改并为修改序列号保证顺序修改。然后所有的副本采取同样的修改顺序。
  - 主chunk 只要被修改后，就可以向master申请更长的租期。租期的申请和批准一般都在心跳包里面。
  - 有时Master会提前取消主chunk的lease。
- 数据流和控制流分开。控制流由client到 主 chunk，然后到二级副本。数据流以管道的方式，选好最优路径后，数据以管道的方式，链式推送数据到chunkserver，保存到内部的LRU缓存中，然后等待控制流，通常选择距离client最近的chunkserver 开始推送。可以更具IP计算出节点的距离。
- 快照+COW。当master收到client的快照请求后，先收回所有chunk的lease.
- 空间回收采用惰性策略
- 每个Chunk服务器都使用Checksum来检查保存的数据是否损坏。



##BigTable(2006)

- 设计目标：适用性广，伸缩性好，高可用，高性能。
-  Bigtable是一个稀疏的、分布式的、持久化存储的多维度排序Map。Map的key是行关键字，列关键字和时间戳；map的value是string( 实际上key 和 value 都是byte数组 )。
  - `(row:string, column:string, time:int64) ----> string`
  - 每个行都可以动态分区，每个分区叫Tablet。
- bigtable 用 GFS 存储日志和数据文件。数据文件的格式是 SSTable ， 是一个持久化的，排序的，不可更改的Map结构。SSTable 是一系列的数据块，并使用索引块定位数据，索引块一般存储在SSTable最后。访问数据时先用二分查找索引块找到数据块的位置，然后再读取数据块。
- bigTable 还依赖一个分布式锁服务Chuuby，用来完成以下任务：
  - 在给定时间内最多只有一个活动的**Master**副本。
  - 存储BigTable数据的自引导指定位置。
  - 查到Table服务器，以及Table服务器失效时进行调整。
  - 存储BigTable 的模式信息（每张表的列簇信息）以及访问控制权限列表。
- BigTable 架构也是 1个Master和 多个 Tablet 服务器。

<img src="/Users/cs/note/images/bigtable01.png" style="zoom:50%" />

- Master服务器的任务如下：
  - 为Tablet服务器分配Tablets 
  - 检测新加入的或者过期失效的Table服务器
  - 对Tablet服务器进行负载均衡
  - 对保存在GFS上的文件进行垃圾收集。
  - 处理对模块相关的修改操作，如建立表和列簇。
- 每个 Tablet 服务器 管理一个Tablet的集合，负责处理对Table的读写操作，以及在Table过大时，对其进行分割。
- Tablet 的位置信息，用一个三层的，类似B+树的结构来存储Tablet  的位置信息。
  - 第一层是存储在chubby中的文件，包含 Root Table的位置信息。Root Table包含了一个特殊的Metadata 表里所有的位置信息。MetaData 表的每个Tablet包含一个用户的Tablet 集合。RootTable实际上是 metadata表的第一个Tablet ， 只不过对它的处理比较特殊，RootTablet 永远不会被分割。这可以保证这棵B+树的深度不会超过3。
    - MetaData表中还存储了二级信息，包括每个Tablet的事件日志，这些信息有助于排插错误和性能分析。
  - 客户端会缓存Tablet的位置信息。
- Tablet的分配，因为Master拥有所有tablet的全局负载信息，会把一个新Tablet分配给足够空闲的Tablet服务器。
-  BigTable使用 Chubby跟踪记录 Tablet服务器的状态。当一个 Tablet服务器启动时，它在 Chubby的一个指定目录下建立一个有唯一性名字的文件，并且获取该文件的独占锁。 Master服务器实时监控着这个目录（服务器目录），因此 Master服务器能够知道有新的 Tablet服务器加入了。如果 Tablet服务器丢失了 Chubby上的独占锁  — 比如由于网络断开导致 Tablet服务器和 Chubby的会话丢失  — 它就停止对 Tablet 提供服务。 只要文件还存在，Tablet 服务器就会试图重新获得对该文件的独占锁；如果文件不存在了，那么Tablet 服务器就不能再提供服务了，它会自行退出。 当 Tablet 服务器终止时（比如，集群的管理系统将运行该Tablet 服务器的主机从集群中移除），它会尝试释放它持有的文件锁，这样一来，Master 服务器就能尽快把Tablet 分配到其它的Tablet 服务器。
- Master的启动步骤
  -  Master服务器从 Chubby获取一个唯一的 Master锁，用来阻止创建其它的 Master服务器实例
  -  Master服务器扫描 Chubby的服务器文件锁存储目录，获取当前正在运行的服务器列表；
  -  Master服务器和所有的正在运行的 Tablet表服务器通信，获取每个 Tablet服务器上 Tablet的分配信息；
  -  Master服务器扫描 METADATA表获取所有的Tablet的集合。在扫描的过程中，当 Master服务器发现了一个还没有分配的 Tablet， Master服务器就将这个 Tablet加入未分配的 Tablet集合等待合适的时机分配。
-  保存现有Tablet 的集合只有在以下事件发生时才会改变：建立了一个新表或者删除了一个旧表、两个Tablet 被合并了、或者一个Tablet 被分割成两个小的Tablet 。Master 服务器可以跟踪记录所有这些事件，因为除了最后一个事件外的两个事件都是由它启动的。Tablet 分割事件需要特殊处理，因为它是由Tablet 服务器启动。。在分割操作完成之后，Tablet 服务器通过在METADATA 表中记录新的Tablet 的信息来提交这个操作；当分割操作提交之后，Tablet 服务器会通知Master 服务器。如果分割操作已提交的信息没有通知到Master 服务器（可能两个服务器中有一个宕机了），Master 服务器在要求Tablet 服务器装载已经被分割的子表的时候会发现一个新的Tablet 。通过对比METADATA 表中Tablet 的信息，Tablet 服务器会发现Master 服务器要求其装载的Tablet 并不完整，因此，Tablet 服务器会重新向Master 服务器发送通知信息。
- Tablet服务器
  - Tablet是持久化保存在GFS上的。更新操作提交到commit（保存REDO信息）日志中。最近的更新存放在一个排序的  memtable 缓存中，更早的操作存放在一系列的SStable中。
  -  当对Tablet 服务器进行写操作时，Tablet 服务器首先要检查这个操作格式是否正确、操作发起者是否有执行这个操作的权限。权限验证的方法是通过从一个Chubby 文件里读取出来的具有写权限的操作者列表来进行验证（这个文件几乎一定会存放在Chubby 客户缓存里）。 成功的修改操作会记录在提交日志里。可以采用批量提交方式 来提高包含大量小的修改操作的应用程序的吞吐量。 当一个写操作提交后，写的内容插入到memtable 里面。
- Compactions
  -  随着写操作的执行，memtable 的大小不断增加。当memtable 的尺寸到达一个门限值的时候，这个memtable 就会被冻结，然后创建一个新的memtable ；被冻结住memtable 会被转换成SSTable ，然后写入GFS。称这种Compaction行为为Minor Compaction。它有两个作用：
    -  shrink（收缩） tablet服务器使用的内存.
    -  在服务器灾难恢复过程中，减少必须从提交日志里读取的数据量。
  - 因为每一次Minor Compaction 都会产生一个新的SSTable ， 如果SSTable过多，读操作需要合并多个SSTable的更新来读取最新数据，从而降低了读性能。所以，需要定期在后台执行  Merging Compaction 合并 多个SSTable 和 memtable 的内容，只要Merging Compaction完成，参加合并的SSTable 和 memtable 就可以删除了。
  -  合并所有的SSTable 并生成一个新的SSTable 的Merging Compaction 过程叫作Major Compaction。 由非Major Compaction 产生的SSTable 可能含有特殊的删除条目。 而Major Compaction 过程生成的SSTable 不包含已经删除的信息或数据。Bigtable 循环扫描它所有的Tablet ，并且定期对它们执行Major Compaction 。Major Compaction 机制允许Bigtable 回收已经删除的数据占有的资源，并且确保BigTable 能及时清除已经删除的数据。
- 优化
  - 局部群组， 序可以将多个列族组合成一个局部性群族。 对Tablet 中的每个局部性群组都会生成一个单独的SSTable 。将通常不会一起访问的列族分割成不同的局部性群组可以提高读取操作的效率。 此外，可以以局部性群组为单位设定一些有用的调试参数。比如，可以把一个局部性群组设定为全部存储在内存中。 在Bigtable 内部，利用这个特性提高METADATA 表中具有位置相关性的列族的访问速度。
  - 压缩。客户端可以控制一个 局部群组 的 SSTable 是否需要压缩，如要要压缩，以什么格式压缩，每个 SSTable的数据块都是由客户端指定的压缩格式来压缩。虽然分块压缩的压缩率较低，但是在读取SSTable的一小部分数据时就只用解压相应的数据块而不是全部SSTable 。 客户端使用了“ 两遍” 的、可定制的压缩方式。
  - 通过缓存提高读操作的性能。Tablet 服务器采用二级缓存策略，第一级缓存是Key-value对；第二级缓存是Block（SSTable的数据块）。
  - Bloom过滤器。一个读操作需要读取多个SSTable来查找最新数据。如果SSTable不在内存中，就需要多次访问磁盘。可以让客户端对特定的局部群组的SSTable指定Bloom过滤器，如果一个SSTable没有包含要访问的行和列，就可以不用访问该SSTable了。
  - Commit 日志的实现。设置为每个Tablet服务器一个 Commit日志文件。把修改操作的日志以append的方式写入同一个日志文件。所以一个实际的日志文件中包含多个Tablet的修改记录。
    - 在Tablet服务器宕机后，它加载的Tablet 将会被移到很多其它的Tablet 服务器上，每个Tablet服务器都装载几个Tablet，但每个Tablet服务器都需要读宕机服务器的Commit日志，并全部扫描（需要执行多次seek ）后执行相关Tablet的恢复操作，这样效率很低。为了提高效率， Master会将日志按照关键字（table，row，log sequence number）进行排序，这样同一个Tablet的修改操作就连续存放在了一起。因此，每个Tablet的恢复就只需要一次seek操作然后顺序读取相关日志就能完成恢复。  
    - 对日志进行排序，可以先将日志分割成64M的段，然后在不同的Tablet服务器对段进行并行排序，由Master服务器协同处理。
  - Tablet恢复加速。当移动一个Tablet到另一个Tablet服务器时，源Tablet服务器会对这个Table做一次 Minor Compaction。  这个Compaction 操作减少了Tablet 服务器的日志文件中没有归并的记录，从而减少了恢复的时间。Compaction 完成之后，该服务器就停止为该Tablet 提供服务。 在卸载Tablet 之前，源Tablet 服务器还会再做一次（通常会很快）Minor Compaction ，以消除前面在一次压缩过程中又产生的未归并的记录。第二次Minor Compaction 完成以后，Tablet 就可以被装载到新的Tablet 服务器上了，并且不需要从日志中进行恢复。
  - 利用不变形。 memtable 是唯一一个能被读和写操作同时访问的可变数据结构。为了减少在读操作时的竞争，我们对内存表采用COW(Copy-on-write) 机制，这样就允许读写操作并行执行。 
    - 因为SSTable 是不变的，因此，我们可以把永久删除被标记为“ 删除” 的数据的问题，转换成对废弃的 SSTable 进行垃圾收集的问题了。每个Tablet 的SSTable 都在METADATA 表中注册了。Master 服务器采用“ 标记- 删除” 的垃圾回收方式删除SSTable 集合中废弃的SSTable
    -  SSTable 的不变性使得分割Tablet 的操作非常快捷。我们不必为每个分割出来的Tablet 建立新的SSTable 集合，而是共享原来的Tablet 的SSTable 集合。



## Dynamo(2007)

http://www.cnblogs.com/foxmailed/archive/2012/01/11/2318650.html

* 关键技术
  * data partitioned : 一致性哈希
  * 数据一致性：对象版本（MVCC ？）
  * 副本间的一致性：quorum-like 和去中心化的副本同步协议
  * 故障检测和成员协议： 基于gossip的分布式故障检测和成员关系







##Megastore（2011）







# 分布式计算

## MapReduce(2004)













## Dremel(2010)

https://searchdatabase.techtarget.com.cn/7-20806/













## presto

Presto是一个开源的分布式SQL查询引擎



## Apache Kylin





## Google F1

<img src="./images/F1_architecture.png" style="zoom:50%" />

