# 入门

http://www.jianshu.com/p/0199762cf258

https://www.zhihu.com/question/22610352

https://www.zhihu.com/question/36514327

http://tips.codekiller.cn/2017/05/17/maglev_describe/

https://zhuanlan.zhihu.com/p/22360384

利用DNS实现负载均衡，就是在DNS服务器配置多个A记录，不同的DNS请求会解析到不同的IP地址。大型网站一般使用DNS作为第一级负载均衡。缺点是DNS生效时间略长，扩展性差。



LVS属于四层负载均衡,工作在tcp/ip协议栈上,通过修改网络包的ip地址和端口来转发, 由于效率比七层高,一般放在架构的前端.



七层的负载均衡有阿里的**Tengine**,nginx, haproxy, apache等, 工作在应用层,因此可以将HTTP请求等应用数据发送到具体的应用服务器,如将图片请求转发到特定的服务器上,总之可以做到更智能的负载均衡*，这些功*能在四层负载均衡上不好实现,一般放在架构的后面位置*,布置在应用服务器前面



[Google Maglev 牛逼的网络负载均衡器](http://tips.codekiller.cn/2017/05/17/maglev_describe/)  工作在三层(IP层)的网络负载均衡器

[UCloud Vortex](https://zhuanlan.zhihu.com/p/22360384)

[MGW——美团点评高性能四层负载均衡](https://zhuanlan.zhihu.com/p/28428681)



# LVS

[参考](http://blief.blog.51cto.com/6170059/1745134)

工作方式有三种：

* nat模式（LVS/NAT),
* 直接路由模式（ LVS/DR）
* ip隧道模式(LVS/TUN)。



* LVS/NAT模式

  NAT用法本来是因为网络IP地址不足而把内部保留IP地址通过映射转换成公网地址的一种上网方式。

  如果把NAT的过程稍微变化,，就可以成为负载均衡的一种方式，原理其实就是把从客户端发来的IP包的IP头目的地址在DR上换成服务器集群其中一台REALSERVER的IP地址并发至此 REALSERVER，而REALSERVER则在处理完成后把数据经过DR主机发回给客户端，DR在这个时候再把数据包的原IP地址改为DR接口上的 IP地址即可期间，**无论是进来的流量,还是出去的流量,都必须经过DR**，故DR可能成为性能瓶颈。

* LVS/DR模式

  每个Real Server上都有两个IP：VIP和RIP，但是VIP是隐藏的，就是不能提高解析等功能，只是用来做请求回复的源IP的，Director上只需要一个网卡，然后利用别名来配置两个IP：VIP和DIP，在DIR接收到客户端的请求后，DIR根据负载算法选择一台rs sever的网卡mac作为客户端请求包中的目标mac，通过arp转交给后端rs serve处理，后端再通过自己的路由网关回复给客户端

* LVS/TUN 模式

  它的连接调度和管理与VS/NAT中的一样，利用ip隧道技术的原理，即在原有的客户端请求包头中再加一层IP Tunnel的包头ip首部信息，不改变原来整个请求包信息，只是新增了一层ip首部信息，再利用路由原理将请求发给RS server，不过要求的是所有的server必须支持"IPTunneling"或者"IP Encapsulation"协议

三种负载均衡技术比较总结表：

| 要求      | LVS/NAT         | LVS/TUN              | LVS/DR                      |
| ------- | --------------- | -------------------- | --------------------------- |
| RS OS要求 | 任何系统            | 必须支持ip隧道协议，目前只有Linux | 服务节点需支持vip并能够禁用该设备上的arp响应功能 |
| 网络要求    | 拥有私有ip的局域网环境    | 拥有合法ip的局域网或广域网       | 拥有合法ip的局域网，服务节点需与均衡器在同一广播域  |
| 负载节点数   | 10-20个，视均衡器性能而定 | >100                 | >100                        |
| RS  网关  | 均衡器             | 自有的网关路由              | 自有的网关路由                     |
| 优点      | 地址和端口转换         | Wan/Lan环境加密数据        | **性能最高**                    |
| 缺点      | 效率低             | 需要隧道支持               | 不能跨域LAN                     |



# Google Maglev

参考：

[Google 是如何做负载均衡的？](http://mp.weixin.qq.com/s/2NQcwukKoHR4eFXZNtIb1w)

[Google Maglev 牛逼的网络负载均衡器](https://segmentfault.com/a/1190000009565788)

工作在三层(IP层)的网络负载均衡器, 它是一个运行在普通的 Linux 系统上的巨大的分布式系统, 并且可以简单平滑的伸缩后端服务器数量.

<img src="https://raw.githubusercontent.com/crazycs520/images/master/slb.png.png" style="zoom:30%" />

​	以我们访问 www.google.com 为例，第一步 DNS 服务器会根据请求的位置返回一个离请求地理位置最近的 VIP 地址，先在 DNS 这一层做一个横向扩展。接下来请求达到 VIP 对应的路由器，**路由器通过 ECMP(等价路由) 协议**，可以将请求平均分配到下面对等的多个负载均衡器上，这样在路由器这一层做了个负载均衡，让后面的负载均衡器也实现了横向扩展。再往下是一个类似于 LVS 中 DR 模式的分发，负载均衡器将请求包转发给服务器同时将源地址改为客户请求时的地址，服务器响应时将响应包的源地址改为 VIP 的地址直接打给路由器而不通过负载均衡器来降低负载均衡器压力。

​	简单的说虽然 LVS 已经做到 Linux 内核里了，但是在 Google 看来 Linux 是性能的瓶颈，到 LVS 之前还要经过完整的 TCP/IP 协议栈以及内核的一系列 filter 模块，而这些对于转发来说是没有必要的。于是 Google 的做法就是简单粗暴的绕过内核，把 Maglev 直接架在网卡上对接网卡的输入和输出队列，来一个数据包也不需要完整的 TCP/IP 协议栈的解析，进来的包只要分析前几个字节，拿出源地址，源端口，目标地址，目标端口和协议号这个五元组对于转发来说就已经足够了。剩下的诸如 payload，序列号之类的东西统统不关心直接塞到网卡输出口给后面就行了。

​	通常的负载均衡器都是一个单点，而 Maglev 是一个集群，集群就会碰到很多的问题。严格来说负载均衡器不能算是一个无状态的服务，因为 TCP 连接本身是有状态的，一组会话内的请求包必须转发到相同的后端服务器，不然服务器端的 TCP 会话就乱套了。对于单点的负载均衡器来说很好解决，记录个转发表里面有每个数据包的五元组和它第一转发到哪台机器，来一个新的数据包查这个表就知道给谁了。而像 Maglev 这样的集群数据包是通过路由器 ECMP 随机分发的，第一个数据包是这个 Maglev node 处理，下一个就不知道去哪个 Maglev node 了。而且集群就会涉及滚动式的更新和随机的故障,这样本机的转发表也就很可能会丢失。

​	而 Google  Maglev 直接就上了*一致性哈希*这个大杀器。这样的话可以直接通过五元组散列到后端的一台固定服务器，这样硬生生的把有状态服务做成了无状态,如此一来 Maglev 层面个就可以随意的更新，上线下线了。顺便的一个好处就是后端增加下线服务器都只会影响到当前这台机器所处理的连接不会造成所有连接的 rehash。当然只用一个普通的一致性哈希算法也没啥意思，Google 为了自己的需求专门写了个 Maglev Hashing 。 选择好后端之后, Maglev 会将该5元组对应的后端记录在自己的 connection tracking 表中, 等到下个数据包来的时候, 只要根据包的5元组直接查询这个 connection tracking 表即可. 那为什么有了一致性哈希选择后端, 还要有 connection tracking 呢? 这是因为当一个 VIP 对应的 Service Endpoint 扩容或者缩容的时候, 一致性哈希选择的结果会发生变化, 这样会导致同一连接上的包选择的后端不一致, 造成网络错误.



# UCloud Vortex

参考：

[https://zhuanlan.zhihu.com/p/22360384](https://zhuanlan.zhihu.com/p/22360384)

Vortex通过ECMP集群和一致性哈希来实现极致程度的可靠性。



#负载均衡用到的算法

轮询

权值轮询

[一致性哈希](https://zhuanlan.zhihu.com/p/24440059)